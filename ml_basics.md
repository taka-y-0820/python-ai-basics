## Day 3: 機械学習のコアとなる基礎理論

このドキュメントは、機械学習の分類と基本モデルに関する理論的基礎をまとめたものです。

---

### 1. 教師あり学習 (Supervised Learning)

**定義**: モデルは、入力データ $X$ と、それに対応する**正解ラベル $Y$** のペアを用いて訓練されます。モデルの目的は、両者の関係性を学習し、未知の入力 $X$ に対して正確な出力 $Y$ を予測することです。

| タスク                    | 目的                                           | 予測される出力 $Y$                        | コード解析での応用例                                                             |
| :------------------------ | :--------------------------------------------- | :---------------------------------------- | :------------------------------------------------------------------------------- |
| **分類 (Classification)** | データが**どのカテゴリ**に属するかを予測する。 | 離散的なカテゴリ（例: Yes/No、安全/脆弱） | 特定のコードスニペットが**「セキュリティ脆弱性を含むか（Yes/No）」**を予測する。 |
| **回帰 (Regression)**     | **連続した数値**を予測する。                   | 連続値（例: 3.5 秒、100 行）              | 関数の**推定実行時間**や**リソース使用量**を予測する。                           |

---

### 2. 教師なし学習 (Unsupervised Learning)

**定義**: モデルは、**ラベル付けされていないデータ**（$X$ のみ）を用いて訓練されます。目的は、データ自身の**内部構造**や**隠れたパターン**、統計的な関係性を見つけ出すことです。

| 手法                                    | 目的                                                       | 利用例                                                                 |
| :-------------------------------------- | :--------------------------------------------------------- | :--------------------------------------------------------------------- |
| **クラスタリング (Clustering)**         | 類似性の高いデータ同士を**グループ化**する。               | 顧客のセグメンテーション、コードベース内の**類似ロジックの自動検出**。 |
| **次元削減 (Dimensionality Reduction)** | データの情報を保ちつつ、**特徴量の数（次元）を削減**する。 | 大規模データの可視化、モデルの計算負荷軽減。                           |
| **異常検知 (Anomaly Detection)**        | 通常のパターンから大きく外れたデータ（外れ値）を特定する。 | サーバーの不正アクセス監視、ログデータの異常トラフィック検知。         |

---

### 3. 線形回帰 (Linear Regression) の基礎

線形回帰は、教師あり学習の「回帰」タスクの最も基本的なモデルです。予測値が特徴量の線形結合（直線的な関係）で表現されます。

線形回帰モデルの基本の式は以下の通りです。

$$y = \beta_0 + \beta_1 x_1 + \epsilon$$

| 記号           | 用語 (英語名)                               | 役割                                                                                    |
| :------------- | :------------------------------------------ | :-------------------------------------------------------------------------------------- |
| **$y$**        | **目的変数 (Target Variable)**              | モデルが予測したい**連続した数値**。                                                    |
| **$\beta_0$**  | **切片 (Intercept)**                        | $x_1$がゼロのときの$y$の予測値（ベースライン）。                                        |
| **$\beta_1$**  | **係数 (Coefficient)**                      | 説明変数 $x_1$ が 1 単位変化したときの、$y$の変化量（傾き）。**特徴量の重要度**を示す。 |
| **$x_1$**      | **説明変数 (Feature/Explanatory Variable)** | $y$を予測するために**利用する入力データ**。                                             |
| **$\epsilon$** | **誤差項 (Error Term)**                     | モデルが説明できない、ランダムなノイズや測定誤差。                                      |

### 補足: ロジスティック回帰の数学的根拠 (シグモイド関数とネイピア数)

ロジスティック回帰が分類問題を解けるのは、線形の予測結果を非線形の「確率」に変換する**シグモイド関数**と、その基盤となる**ネイピア数 ($e$)** の特性によるものです。

#### 1. シグモイド関数 (Sigmoid Function) の役割

シグモイド関数は、入力 $z$ を**必ず 0 から 1 の範囲**に変換し、予測結果を**確率**として解釈できるようにする、S 字型の活性化関数です。

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

| 記号            | 意味                             | 変換範囲                | 役割                                                         |
| :-------------- | :------------------------------- | :---------------------- | :----------------------------------------------------------- |
| **$\sigma(z)$** | **予測確率 ($\hat{p}$)**         | $0 < \hat{p} < 1$       | 分類結果の信頼度（確率）を示す。                             |
| **$z$**         | **ロジット (Logit)**             | $-\infty < z < +\infty$ | 特徴量と係数による線形結合の結果。オッズの自然対数と等しい。 |
| **$e$**         | **ネイピア数** ($\approx 2.718$) | -                       | 自然対数の底。連続的な変化を扱うために不可欠。               |

---

#### 2. ロジットからシグモイドへの導出過程

ロジスティック回帰の定義は、オッズの自然対数（ロジット）が線形モデルに等しいというものです。シグモイド関数は、この関係を「確率 $p$」について解き直した**逆変換**です。

**① ロジットの定義**:
$$z = \ln\left(\frac{p}{1-p}\right)$$

**② 指数関数による対数の解除 ($e^{\ln(A)}=A$ を利用)**:
$$e^z = \frac{p}{1-p}$$

**③ $p$ について解く（途中省略）**:
$$p = \frac{1}{1 + e^{-z}}$$

この逆変換により、線形の結果 $z$ から、最終的な確率 $p$ を直接出力できます。

---

#### 3. 指数と対数、ネイピア数の基本

| 概念                 | 定義                                                                                                                                 | 目的                                                                                         |
| :------------------- | :----------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------- |
| **指数 (べき乗)**    | 底 $b$ を指数 $x$ の回数だけ**繰り返し掛ける**操作 ($b^x = A$)。                                                                     | 成長や減衰の計算。                                                                           |
| **対数 (Logarithm)** | 底 $b$ を**何乗すれば $A$ になるか**という指数 $x$ を求める操作 ($\log_b A = x$)。                                                   | 変化を線形に扱うための変換。                                                                 |
| **ネイピア数 ($e$)** | **自然対数の底** ($\approx 2.71828$)。**変化率が現在の値に比例する**という、最もシンプルな連続的成長を表現するために使用される定数。 | ロジスティック回帰において、線形モデルの結果を数学的に意味のある確率に変換するために不可欠。 |
