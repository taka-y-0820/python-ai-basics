## Day 3: 機械学習のコアとなる基礎理論

このドキュメントは、機械学習の分類と基本モデルに関する理論的基礎をまとめたものです。

---

### 1. 教師あり学習 (Supervised Learning)

**定義**: モデルは、入力データ $X$ と、それに対応する**正解ラベル $Y$** のペアを用いて訓練されます。モデルの目的は、両者の関係性を学習し、未知の入力 $X$ に対して正確な出力 $Y$ を予測することです。

| タスク                    | 目的                                           | 予測される出力 $Y$                        | コード解析での応用例                                                             |
| :------------------------ | :--------------------------------------------- | :---------------------------------------- | :------------------------------------------------------------------------------- |
| **分類 (Classification)** | データが**どのカテゴリ**に属するかを予測する。 | 離散的なカテゴリ（例: Yes/No、安全/脆弱） | 特定のコードスニペットが**「セキュリティ脆弱性を含むか（Yes/No）」**を予測する。 |
| **回帰 (Regression)**     | **連続した数値**を予測する。                   | 連続値（例: 3.5 秒、100 行）              | 関数の**推定実行時間**や**リソース使用量**を予測する。                           |

---

### 2. 教師なし学習 (Unsupervised Learning)

**定義**: モデルは、**ラベル付けされていないデータ**（$X$ のみ）を用いて訓練されます。目的は、データ自身の**内部構造**や**隠れたパターン**、統計的な関係性を見つけ出すことです。

| 手法                                    | 目的                                                       | 利用例                                                                 |
| :-------------------------------------- | :--------------------------------------------------------- | :--------------------------------------------------------------------- |
| **クラスタリング (Clustering)**         | 類似性の高いデータ同士を**グループ化**する。               | 顧客のセグメンテーション、コードベース内の**類似ロジックの自動検出**。 |
| **次元削減 (Dimensionality Reduction)** | データの情報を保ちつつ、**特徴量の数（次元）を削減**する。 | 大規模データの可視化、モデルの計算負荷軽減。                           |
| **異常検知 (Anomaly Detection)**        | 通常のパターンから大きく外れたデータ（外れ値）を特定する。 | サーバーの不正アクセス監視、ログデータの異常トラフィック検知。         |

---

### 3. 線形回帰 (Linear Regression) の基礎

線形回帰は、教師あり学習の「回帰」タスクの最も基本的なモデルです。予測値が特徴量の線形結合（直線的な関係）で表現されます。

線形回帰モデルの基本の式は以下の通りです。

$$y = \beta_0 + \beta_1 x_1 + \epsilon$$

| 記号           | 用語 (英語名)                               | 役割                                                                                    |
| :------------- | :------------------------------------------ | :-------------------------------------------------------------------------------------- |
| **$y$**        | **目的変数 (Target Variable)**              | モデルが予測したい**連続した数値**。                                                    |
| **$\beta_0$**  | **切片 (Intercept)**                        | $x_1$がゼロのときの$y$の予測値（ベースライン）。                                        |
| **$\beta_1$**  | **係数 (Coefficient)**                      | 説明変数 $x_1$ が 1 単位変化したときの、$y$の変化量（傾き）。**特徴量の重要度**を示す。 |
| **$x_1$**      | **説明変数 (Feature/Explanatory Variable)** | $y$を予測するために**利用する入力データ**。                                             |
| **$\epsilon$** | **誤差項 (Error Term)**                     | モデルが説明できない、ランダムなノイズや測定誤差。                                      |

### 補足: シグモイド関数 (Sigmoid Function) の詳細

ロジスティック回帰の分類の核となる関数であり、線形モデルの結果を「確率」に変換します。

#### 1. 数学的な役割

線形回帰の出力 $z = (\beta_0 + \beta_1 x_1 + \dots)$ は $-\infty$ から $+\infty$ までの任意の値をとりますが、分類タスクで必要な **0 から 1 の範囲**に値を押し込めるのがシグモイド関数の役割です。

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

| 記号            | 意味                     | 変換範囲                | 補足                                             |
| :-------------- | :----------------------- | :---------------------- | :----------------------------------------------- |
| **$\sigma(z)$** | **予測確率 ($\hat{p}$)** | $0 < \hat{p} < 1$       | 最終的にモデルが出力する、カテゴリに属する確率。 |
| **$z$**         | **線形スコア**           | $-\infty < z < +\infty$ | 特徴量と係数の線形結合の結果。                   |

#### 2. 関数の特性と重要性

- **確率への変換**: $z$ がどんなに大きな値になっても、出力 $\sigma(z)$ は決して 1 を超えず、どんなに小さな値になっても 0 を下回りません。これにより、予測を常に「確率」として解釈できます。
- **分類境界**: $z=0$ のとき、$\sigma(z)=0.5$（50%）となり、これが**分類の境界線**となります。モデルは、この境界を学習することでデータを 2 つのカテゴリに分類します。
- **非線形性**: この S 字カーブ（非線形な変換）を導入することで、ロジスティック回帰は線形モデルでありながら、**分類問題**を解く能力を獲得します。
