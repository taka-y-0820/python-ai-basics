## Day 7: Transformer モデルと Attention 機構の基礎

このドキュメントは、LLM (大規模言語モデル) の核となる **Transformer アーキテクチャ**と、その心臓部である **自己注意機構 (Self-Attention)** の理論的基盤をまとめたものです。

---

### 1. Transformer アーキテクチャの優位性

Transformer は、従来の RNN や LSTM が抱えていた「逐次処理（順序依存性）」の問題を解決し、長距離の文脈理解と高速な学習を可能にしました。

| 課題           | 従来の RNN/LSTM                                | Transformer による解決                                                  |
| :------------- | :--------------------------------------------- | :---------------------------------------------------------------------- |
| **学習速度**   | 逐次処理のため、処理が遅い。                   | **並列処理** (全ての単語を同時に処理) を可能にし、学習を高速化。        |
| **文脈理解**   | 長い文脈の情報が薄れる（長距離依存性の問題）。 | **Self-Attention** により、どの単語とも直接関係性を計算可能。           |
| **順序の認識** | 自動で順序を認識する。                         | **並列処理の弊害**で順序を失う。**Positional Encoding**で明示的に解決。 |

#### Positional Encoding（位置エンコーディング）

単語の順序情報を追加するための特殊なベクトルです。これにより、Transformer は並列処理を行いつつも、各単語が文章のどこに位置するかを認識できます。

---

### 2. 自己注意機構 (Self-Attention Mechanism) の核心

Self-Attention は、入力シーケンス内の各単語が、他のすべての単語とどれだけ関連しているかという**重み（注意スコア）**を計算し、文脈情報を集約する仕組みです。

#### Query, Key, Value (QKV) の役割

Attention の計算は、入力ベクトルを以下の 3 つのベクトル（テンソル）に変換することから始まります。

| ベクトル                   | 役割（意味）                                               | 計算への関与                                      |
| :------------------------- | :--------------------------------------------------------- | :------------------------------------------------ |
| **$Q$ (Query / 質問)**     | 「この単語が、他のどの単語に注目すべきか」という**質問**。 | **類似度計算** (Query $\cdot$ Key) の入力となる。 |
| **$K$ (Key / 鍵)**         | 「この単語が持っている情報」という**インデックス**。       | **類似度計算** (Query $\cdot$ Key) の入力となる。 |
| **$V$ (Value / 情報本体)** | 注意スコアに応じて**集約される情報本体**。                 | 最終的な文脈情報（Weighted Sum）の要素となる。    |

#### 数式による計算の流れ

Attention の最終的な出力は、以下の 3 つのステップを経て計算されます。

1.  **関連度スコアの計算**: $Q$ と $K$ の内積（ドット積）を計算します。
    $$\text{Score} = Q K^T$$

2.  **重み（確率）の計算**: スコアをスケーリングし、Softmax 関数を通して合計が 1 になる確率（Attention Weight）に変換します。
    $$\text{Attention Weight} = \text{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right)$$

3.  **情報集約 (重み付き和)**: 計算された重み (Attention Weight) を $V$ に掛け合わせ、重み付きの合計（文脈情報）を出力します。
    $$\text{Attention}(Q, K, V) = \text{Attention Weight} \cdot V$$

この集約された情報が、次の層に渡され、LLM の驚異的な**文脈理解能力**を支えています。
